{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import statements\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy.linalg.blas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(dataset): \n",
    "    #include checking for if it already exists\n",
    "    origin = ( 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz' )\n",
    "    dataset = 'MNIST'\n",
    "    import urllib\n",
    "    urllib.urlretrieve(origin, dataset)\n",
    "    '''downloads the file to the current working directoty and makes a file in that one'''\n",
    "    f = gzip.open(dataset, 'rb')#looks in current directy on path\n",
    "    train_set, test_set, val_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    def shared_dataset(data_xy, borrow = True):\n",
    "        data_x, data_y = data_xy \n",
    "        shared_x = theano.shared(numpy.asarray(data_x, dtype = theano.config.floatX), \n",
    "                                 borrow = borrow )\n",
    "        shared_y = theano.shared(numpy.asarray(data_y, dtype = theano.config.floatX), \n",
    "                                 borrow = borrow )\n",
    "        return shared_x, T.cast(shared_y, 'int32' )\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(val_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval \n",
    "#must make the data itself a shared varaible so it can be used with theano functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitNum(dataset):# took out batch size\n",
    "    datasets=dataset\n",
    "    train_set_x , train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]#this is now all of them\n",
    "    return train_set_x , train_set_y, valid_set_x, valid_set_y, n_train_batches, n_valid_batches #throw away the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TensorType(float64, matrix)>,\n",
       " Elemwise{Cast{int32}}.0,\n",
       " <TensorType(float64, matrix)>,\n",
       " Elemwise{Cast{int32}}.0,\n",
       " 50000,\n",
       " 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_data('MNIST')\n",
    "splitNum(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hiddenLayer(object):\n",
    "    #everything in the class is callable from the when it is instantiated so. \n",
    "    #both w and b need to be shared varaibles because they are interacting with data which is in a numpy array\n",
    "    \n",
    "    def __init__(self,rng, input, nin,nout,W=None, b = None, activation = T.tanh):\n",
    "        self.input = input\n",
    "        \n",
    "        if W is None:\n",
    "            W_values = np.asarray(rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (nin + nout)),\n",
    "                    high=numpy.sqrt(6. / (nin + nout)),\n",
    "                    size=(nin,nout))\n",
    "                                  , dtype = theano.config.floatX )#say what the theano datatype is\n",
    "            W = theano.shared(value = W_values, name = 'W' , borrow = True)#create the shared variable\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((nout,) , dtype = theano.config.floatX)#it is important that the dtype is the theano value \n",
    "            b = theano.shared(value = b_values , name = 'b', borrow = True)\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        lin_output = T.dot(input,self.W) + self.b# do a matrix multiplication before the function\n",
    "        self.output = (activation(lin_output))#take the tan.h of the input as the out put of the layer.\n",
    "        \n",
    "        self.params = [self.W, self.b] #really important to note these and the output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    import numpy as np\n",
    "    #shared varaibles are defined here because they are coming from input data which is in numpy arrays\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        '''initialize with zeroes a matrix of ninxnout'''\n",
    "        self.W = theano.shared( value = numpy.zeros((n_in,n_out), dtype = theano.config.floatX), \n",
    "                               name = 'W', borrow = True)\n",
    "        self.b = theano.shared(value = numpy.zeros((n_out,), dtype = theano.config.floatX), #comma after nout\n",
    "                               name = 'b', borrow = True)\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input,self.W) + self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis = 1)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input\n",
    "    def negative_log_likelihood(self,y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "    def errors(self,y):#took out error checking\n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object): #note how these are all definition, nothing has been instanctiated yet\n",
    "    def __init__(self,nin,nout,input,nhidden,rng):\n",
    "        #each of the self.thing here is callable from the instance of mlp. so what doe i need out of mlp\n",
    "        # i need cost_method,l1,l2,error,params, hidden layer, logisitc layer\n",
    "        self.Hidden_layer = hiddenLayer(rng= rng, input =input, nin= nin, nout = nhidden ) #use the hidden layer class\n",
    "        self.Logistic_layer = LogisticRegression(input = self.Hidden_layer.output, n_in = nhidden, n_out = nout)#use the logistic layer class\n",
    "        #self.l1 = abs(self.Hidden_layer.W) + abs(self.Logistic_layer.W)#get regularizations\n",
    "        #self.l2 =  (self.Hidden_layer.W ** 2).sum() + (self.Logistic_layer.W ** 2).sum()\n",
    "        self.neg_log_likelihood = (self.Logistic_layer.negative_log_likelihood)\n",
    "        self.errors = self.Logistic_layer.errors\n",
    "        self.params =  self.Logistic_layer.params + self.Hidden_layer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create theano functions which is anoulagous to using the c ompilier in a lot of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def theanofunctions():\n",
    "    ds = load_data('MNIST')\n",
    "    train_set_x , train_set_y, valid_set_x, valid_set_y, n_train_batches, n_valid_batches = splitNum(ds)\n",
    "    ##define theano symbolic varaibles    index = T.lscalar()\n",
    "    x = T.matrix('x')#declare varaibles that will be stand in for data in theano functions\n",
    "    y= T.ivector('y')\n",
    "    index = T.lscalar()\n",
    "    #took out index because no minibatches\n",
    "    #l1_reg=0.00 took out regularization\n",
    "    #l2_reg=0.0001\n",
    "    rng = numpy.random.RandomState(1234) #3 seed the random state\n",
    "    #the thing is actually being created here\n",
    "    classifier = MLP(input = x, nin = 28*28, nhidden = 5000, nout =10 , rng = rng) # use the MLP class#nhidden is hard coded in\n",
    "    cost = classifier.neg_log_likelihood(y)\n",
    "    \n",
    "    #def createmodel(outputs, batch_size,updates, datax, datay):\n",
    "     #   model = theano.function(inputs = [index] , outputs = outputs , updates = updates, givens = {x: datax* index , y: datay*index}\n",
    "                                \n",
    "      #  return model\n",
    "    \n",
    "#took out test model\n",
    "    #valid_model = createmodel(classifier.errors(y),n_valid_batches,None)  \n",
    "    valid_model = theano.function(inputs = [] , outputs = classifier.errors(y) , updates = None, \n",
    "                                  givens = {x: valid_set_x , y: valid_set_y}, on_unused_input = 'warn')\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params] # take the gradient with respect to each variable, but currently no varaible in 2 of the places\n",
    "    updates = [(param, param - 0.02 * gparam) for param, gparam in zip(classifier.params, gparams)] # when this is minus the percentage goes dow\n",
    "    #hard coded leeaerning rate\n",
    "    \n",
    "    train_model = theano.function(inputs = [] , outputs = classifier.errors(y) , updates = updates, \n",
    "                                  givens = {x: train_set_x , y: train_set_y} , on_unused_input = 'warn')\n",
    "   \n",
    "    return train_model, valid_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_set_x , train_set_y, valid_set_x, valid_set_y, n_train_batches, n_valid_batches = splitNum(ds)\n",
    "train_model, valid_mode = theanofunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start looping\n",
      "start looping\n",
      "cycle: 0\n",
      "cost\n",
      "0.90136\n",
      "cycle: 1\n",
      "cost\n",
      "0.31776\n",
      "cycle: 2\n",
      "cost\n",
      "0.3072\n",
      "cycle: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-447dabdc56bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     'obtained at iteration %i, with test performance %f %%') %\n\u001b[0;32m     46\u001b[0m     (best_validation_loss * 100., best_iter + 1, best_validation_loss * 100.))#this is measuring the losses, so cost goes fown losses go down\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mtrain_mlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-447dabdc56bb>\u001b[0m in \u001b[0;36mtrain_mlp\u001b[1;34m(learning_rate, L1_reg, L2_reg, n_epochs, dataset, batch_size, n_hidden)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcycles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cycle: %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcycles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mavg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cost\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.7.0-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=2,\n",
    "         dataset='mnist.pkl.gz', batch_size=500, n_hidden=100):\n",
    "    #define hyper paramaters \n",
    "    print(\"start looping\")\n",
    "    ds = load_data('MNIST')\n",
    "    #grab previous\n",
    "    train_set_x , train_set_y, valid_set_x, valid_set_y, n_train_batches, n_valid_batches = splitNum(ds)\n",
    "    train_model, valid_model = theanofunctions()\n",
    "    ##paramaters\n",
    "    #patience = 10  # look as this many examples regardless\n",
    "    #patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    #improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = 5\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    #test_score = 0.\n",
    "    #epoch = 0\n",
    "    ###do the training\n",
    "    done_looping = True\n",
    "    print(\"start looping\")\n",
    "    for cycles in xrange(100):\n",
    "        print(\"cycle: %d\" %(cycles))\n",
    "        avg_cost = train_model()\n",
    "        print(\"cost\")\n",
    "        print(avg_cost)\n",
    "        iter = cycles\n",
    "        if (iter) %5 == 0: \n",
    "            validation_loss = valid_model()\n",
    "            this_validation_loss = validation_loss\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                best_validation_loss = this_validation_loss\n",
    "                best_iter = iter \n",
    "                #test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "                #test_score = numpy. mean(test_losses)\n",
    "        #if patience <= iter:\n",
    "            #done_looping = True\n",
    "            #break\n",
    "    print(('Optimization complete. Best validation score of %f %% '\n",
    "    'obtained at iteration %i, with test performance %f %%') %\n",
    "    (best_validation_loss * 100., best_iter + 1, best_validation_loss * 100.))#this is measuring the losses, so cost goes fown losses go down\n",
    "train_mlp()\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
