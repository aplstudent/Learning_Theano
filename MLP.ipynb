{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import statements\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the five variables that are passed to each function or class are: self, rng, n_in,n_out, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there are two classes and one function. that's hidden layer, mlp and sgd_test. hidden layer has those 5 paramaters above and weights, b and activation. mlp \n",
    "#has the same 5 plus n_hidden_units, sgd_test has a bunch of hyper paramaters. epochs, learning_rate, l1, l2, batch_size, n_hidden, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(dataset): \n",
    "    #include checking for if it already exists\n",
    "    origin = ( 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz' )\n",
    "    dataset = 'MNIST'\n",
    "    import urllib\n",
    "    urllib.urlretrieve(origin, dataset)\n",
    "    '''downloads the file to the current working directoty and makes a file in that one'''\n",
    "    f = gzip.open(dataset, 'rb')#looks in current directy on path\n",
    "    train_set, test_set, val_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    def shared_dataset(data_xy, borrow = True):\n",
    "        data_x, data_y = data_xy \n",
    "        shared_x = theano.shared(numpy.asarray(data_x, dtype = theano.config.floatX), \n",
    "                                 borrow = borrow )\n",
    "        shared_y = theano.shared(numpy.asarray(data_y, dtype = theano.config.floatX), \n",
    "                                 borrow = borrow )\n",
    "        return shared_x, T.cast(shared_y, 'int32' )\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(val_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval \n",
    "#must make the data itself a shared varaible so it can be used with theano functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitNum(dataset,batch_size):\n",
    "    datasets=dataset\n",
    "    train_set_x , train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    return train_set_x , train_set_y, valid_set_x, valid_set_y, test_set_x, test_set_y, n_train_batches, n_valid_batches,n_test_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hiddenLayer(object):\n",
    "    #everything in the class is callable from the when it is instantiated so. \n",
    "    #both w and b need to be shared varaibles because they are interacting with data which is in a numpy array\n",
    "    \n",
    "    def __init__(self,rng, input, nin,nout,W=None, b = None, activation = T.tanh):\n",
    "        self.input = input\n",
    "        \n",
    "        if W is None:\n",
    "            W_values = np.assarray(rng.uniform, low=-numpy.sqrt(6. / (nin + nout),\n",
    "                    high=numpy.sqrt(6. / (nin + nout)),\n",
    "                    size=(nin, nout)) , dtype = theano.config.floatX, )#say what the theano datatype is\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_value *= 4\n",
    "            W = theano.shard(values = W_values, name = 'W' , borrow = True)#create the shared variable\n",
    "        if b is None:\n",
    "            b_values = numpy.zeroes((nout) , dtype = theano.config.floatX)#it is important that the dtype is the theano value \n",
    "            b = theano.shared(values = b_values , name = 'b', borrow = True)\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        lin_output = T.dot(input,self.W)# do a matrix multiplication before the function\n",
    "        self.output = ( lin_output if activation is None\n",
    "                        else activation(lin_output))#take the tan.h of the input as the out put of the layer.\n",
    "        \n",
    "        self.params = [self.W,self.b]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    import numpy as np\n",
    "    #shared varaibles are defined here because they are coming from input data which is in numpy arrays\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        '''initialize with zeroes a matrix of ninxnout'''\n",
    "        self.W = theano.shared( value = numpy.zeros((n_in,n_out), dtype = theano.config.floatX), \n",
    "                               name = 'W', borrow = True)\n",
    "        self.b = theano.shared(value = numpy.zeros((n_out,), dtype = theano.config.floatX), \n",
    "                               name = 'b', borrow = True)\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input,self.W) + self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis = 1)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input\n",
    "    def negative_log_likelihood(self,y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "    def errors(self,y):\n",
    "        if y.ndim != self.y_pred.ndim: \n",
    "            raise TypeError('y should have the same shape as self.y_pred'('y', y.type, 'yprred',\n",
    "                                                                          self.y_pred.type))\n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self,nin,nout,input,nhidden,rng):\n",
    "        #each of the self.thing here is callable from the instance of mlp. so what doe i need out of mlp\n",
    "        # i need cost_method,l1,l2,error,params, hidden layer, logisitc layer\n",
    "        self.Hidden_layer = hiddenLayer(self, rng, input, nin, nout, nhidden ) #use the hidden layer class\n",
    "        self.Logistic_layer = LogisticRegression(input, nhidden, nout)#use the logistic layer class\n",
    "        #self.l1 = abs(self.Hidden_layer.W) + abs(self.Logistic_layer.W)#get regularizations\n",
    "        #self.l2 =  (self.Hidden_layer.W ** 2).sum() + (self.Logistic_layer.W ** 2).sum()\n",
    "        self.neg_log_likelihood = (self.Logistic_layer.negative_log_likelihood)\n",
    "        self.errors = self.Logistic_layer.errors\n",
    "        self.params =  self.Logistic_layer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-62c11adb186b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def theanofunctions(batch_size,datax,datay, nh =784,learning_rate=0.01):\n",
    "    ##define theano symbolic varaibles    index = T.lscalar()\n",
    "    x = T.matrix('x')#declare varaibles that will be stand in for data in theano functions\n",
    "    y= T.ivector('y')\n",
    "    index = T.lscalar()\n",
    "    #l1_reg=0.00\n",
    "    #l2_reg=0.0001\n",
    "    rng = numpy.random.rand(1234) #3 seed the random state\n",
    "    \n",
    "    classifier = MLP(input = x, nin = 28*28, nhidden = nh, nout =600 , rng = rng) # use the MLP class#nhidden is hard coded in\n",
    "    cost = classifier.neg_log_likelihood(y)\n",
    "    \n",
    "    def createmodel(outputs ,index, datax, datay, batch_size,updates):\n",
    "        model = theano.function(inputs = [index] , outputs = outputs , updates = updates\n",
    "                                , givens = {x : datax[index*batch_size : (index+1) * batch_size], \n",
    "                                                            y : datay[index*batch_size : (index+1) * batch_size]})\n",
    "        return model\n",
    "    \n",
    "    test_model = createmodel(classifier.errors(y), index, test_set_x, test_set_y, 600, None)\n",
    "    valid_model = createmodel(classifier.errors(y) , index, valid_set_x, valid_set_y, 600,None)     \n",
    "    gparams = [T.grad(cost, param) for param in classifier.params] # take the gradient with respect to each variable, but currently no varaible in 2 of the places\n",
    "    updates = [(param, param - learning_rate * gparam) for param, gparam in zip(classifier.params, gparams)] # when this is minus the percentage goes dow\n",
    "    \n",
    "    \n",
    "    train_model = createmodel(cost, index, train_set_x, train_set_y, 600, updates=updates)\n",
    "   \n",
    "    return train_model, test_model, valid_model\n",
    "                \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start looping\n",
      "start looping\n",
      "1\n",
      "minibatch_index: 0\n",
      "cost\n",
      "6.39692965522\n",
      "minibatch_index: 1\n",
      "cost\n",
      "6.35338901237\n",
      "minibatch_index: 2\n",
      "cost\n",
      "6.30395611064\n",
      "minibatch_index: 3\n",
      "cost\n",
      "6.2551945009\n",
      "minibatch_index: 4\n",
      "cost\n",
      "6.21020461273\n",
      "minibatch_index: 5\n",
      "cost\n",
      "6.17672144892\n",
      "minibatch_index: 6\n",
      "cost\n",
      "6.12578822643\n",
      "minibatch_index: 7\n",
      "cost\n",
      "6.0714609173\n",
      "minibatch_index: 8\n",
      "cost\n",
      "6.01452146013\n",
      "minibatch_index: 9\n",
      "cost\n",
      "5.96171487664\n",
      "minibatch_index: 10\n",
      "cost\n",
      "5.88885702652\n",
      "minibatch_index: 11\n",
      "cost\n",
      "5.90883570321\n",
      "minibatch_index: 12\n",
      "cost\n",
      "5.90138338698\n",
      "minibatch_index: 13\n",
      "cost\n",
      "5.77872363524\n",
      "minibatch_index: 14\n",
      "cost\n",
      "5.71699077948\n",
      "minibatch_index: 15\n",
      "cost\n",
      "5.72420924992\n",
      "minibatch_index: 16\n",
      "cost\n",
      "5.6573401943\n",
      "minibatch_index: 17\n",
      "cost\n",
      "5.54616380888\n",
      "minibatch_index: 18\n",
      "cost\n",
      "5.51799641686\n",
      "minibatch_index: 19\n",
      "cost\n",
      "5.52314322857\n",
      "minibatch_index: 20\n",
      "cost\n",
      "5.43685258138\n",
      "minibatch_index: 21\n",
      "cost\n",
      "5.40661477877\n",
      "minibatch_index: 22\n",
      "cost\n",
      "5.36014997343\n",
      "minibatch_index: 23\n",
      "cost\n",
      "5.32358293308\n",
      "minibatch_index: 24\n",
      "cost\n",
      "5.37669354499\n",
      "minibatch_index: 25\n",
      "cost\n",
      "5.31362298676\n",
      "minibatch_index: 26\n",
      "cost\n",
      "5.21821485347\n",
      "minibatch_index: 27\n",
      "cost\n",
      "5.18440193899\n",
      "minibatch_index: 28\n",
      "cost\n",
      "5.1543372886\n",
      "minibatch_index: 29\n",
      "cost\n",
      "5.20000320911\n",
      "minibatch_index: 30\n",
      "cost\n",
      "5.0634549678\n",
      "minibatch_index: 31\n",
      "cost\n",
      "5.06559750408\n",
      "minibatch_index: 32\n",
      "cost\n",
      "4.79291192457\n",
      "minibatch_index: 33\n",
      "cost\n",
      "4.9259687516\n",
      "minibatch_index: 34\n",
      "cost\n",
      "4.80860048166\n",
      "minibatch_index: 35\n",
      "cost\n",
      "4.84727645331\n",
      "minibatch_index: 36\n",
      "cost\n",
      "4.75588645722\n",
      "minibatch_index: 37\n",
      "cost\n",
      "4.7000493853\n",
      "minibatch_index: 38\n",
      "cost\n",
      "4.58092312598\n",
      "minibatch_index: 39\n",
      "cost\n",
      "4.72329292479\n",
      "minibatch_index: 40\n",
      "cost\n",
      "4.59983347976\n",
      "minibatch_index: 41\n",
      "cost\n",
      "4.59172096889\n",
      "minibatch_index: 42\n",
      "cost\n",
      "4.42672148682\n",
      "minibatch_index: 43\n",
      "cost\n",
      "4.56718159259\n",
      "minibatch_index: 44\n",
      "cost\n",
      "4.52614249138\n",
      "minibatch_index: 45\n",
      "cost\n",
      "4.44263074834\n",
      "minibatch_index: 46\n",
      "cost\n",
      "4.321078073\n",
      "minibatch_index: 47\n",
      "cost\n",
      "4.1875636933\n",
      "minibatch_index: 48\n",
      "cost\n",
      "4.25235590405\n",
      "minibatch_index: 49\n",
      "cost\n",
      "4.35511812235\n",
      "minibatch_index: 50\n",
      "cost\n",
      "4.28530776187\n",
      "minibatch_index: 51\n",
      "cost\n",
      "4.33144244073\n",
      "minibatch_index: 52\n",
      "cost\n",
      "4.39060647531\n",
      "minibatch_index: 53\n",
      "cost\n",
      "4.12552945718\n",
      "minibatch_index: 54\n",
      "cost\n",
      "4.1129510483\n",
      "minibatch_index: 55\n",
      "cost\n",
      "4.08165300754\n",
      "minibatch_index: 56\n",
      "cost\n",
      "3.97095487843\n",
      "minibatch_index: 57\n",
      "cost\n",
      "3.92592311659\n",
      "minibatch_index: 58\n",
      "cost\n",
      "3.9632816527\n",
      "minibatch_index: 59\n",
      "cost\n",
      "3.95289281773\n",
      "minibatch_index: 60\n",
      "cost\n",
      "3.82404787363\n",
      "minibatch_index: 61\n",
      "cost\n",
      "3.56714295259\n",
      "minibatch_index: 62\n",
      "cost\n",
      "3.79924061262\n",
      "minibatch_index: 63\n",
      "cost\n",
      "3.78397740781\n",
      "minibatch_index: 64\n",
      "cost\n",
      "3.72285710493\n",
      "minibatch_index: 65\n",
      "cost\n",
      "3.53734949963\n",
      "minibatch_index: 66\n",
      "cost\n",
      "3.71042373341\n",
      "minibatch_index: 67\n",
      "cost\n",
      "3.63508085725\n",
      "minibatch_index: 68\n",
      "cost\n",
      "3.66507312324\n",
      "minibatch_index: 69\n",
      "cost\n",
      "3.74682087505\n",
      "minibatch_index: 70\n",
      "cost\n",
      "3.51525554112\n",
      "minibatch_index: 71\n",
      "cost\n",
      "3.45548324647\n",
      "minibatch_index: 72\n",
      "cost\n",
      "3.40011704055\n",
      "minibatch_index: 73\n",
      "cost\n",
      "3.58380960526\n",
      "minibatch_index: 74\n",
      "cost\n",
      "3.46895745138\n",
      "minibatch_index: 75\n",
      "cost\n",
      "3.44508478203\n",
      "minibatch_index: 76\n",
      "cost\n",
      "3.53828340879\n",
      "minibatch_index: 77\n",
      "cost\n",
      "3.30986993484\n",
      "minibatch_index: 78\n",
      "cost\n",
      "3.31436872026\n",
      "minibatch_index: 79\n",
      "cost\n",
      "3.13315134547\n",
      "minibatch_index: 80\n",
      "cost\n",
      "2.96110454895\n",
      "minibatch_index: 81\n",
      "cost\n",
      "3.22796534426\n",
      "minibatch_index: 82\n",
      "cost\n",
      "3.25920456018\n",
      "2\n",
      "minibatch_index: 0\n",
      "cost\n",
      "3.19765353391\n",
      "minibatch_index: 1\n",
      "cost\n",
      "3.22775347584\n",
      "minibatch_index: 2\n",
      "cost\n",
      "3.01065549834\n",
      "minibatch_index: 3\n",
      "cost\n",
      "2.95406261694\n",
      "minibatch_index: 4\n",
      "cost\n",
      "3.00527181675\n",
      "minibatch_index: 5\n",
      "cost\n",
      "3.1290320115\n",
      "minibatch_index: 6\n",
      "cost\n",
      "3.00113777133\n",
      "minibatch_index: 7\n",
      "cost\n",
      "2.90384350494\n",
      "minibatch_index: 8\n",
      "cost\n",
      "2.81878020705\n",
      "minibatch_index: 9\n",
      "cost\n",
      "2.79428982606\n",
      "minibatch_index: 10\n",
      "cost\n",
      "2.62706983205\n",
      "minibatch_index: 11\n",
      "cost\n",
      "2.99713643457\n",
      "minibatch_index: 12\n",
      "cost\n",
      "3.1603959319\n",
      "minibatch_index: 13\n",
      "cost\n",
      "2.74293862726\n",
      "minibatch_index: 14\n",
      "cost\n",
      "2.67457039565\n",
      "minibatch_index: 15\n",
      "cost\n",
      "2.82861409672\n",
      "minibatch_index: 16\n",
      "cost\n",
      "2.71251427206\n",
      "minibatch_index: 17\n",
      "cost\n",
      "2.53338115688\n",
      "minibatch_index: 18\n",
      "cost\n",
      "2.53771133775\n",
      "minibatch_index: 19\n",
      "cost\n",
      "2.70583300734\n",
      "minibatch_index: 20\n",
      "cost\n",
      "2.57836586508\n",
      "minibatch_index: 21\n",
      "cost\n",
      "2.62688524672\n",
      "minibatch_index: 22\n",
      "cost\n",
      "2.59451443511\n",
      "minibatch_index: 23\n",
      "cost\n",
      "2.61416092925\n",
      "minibatch_index: 24\n",
      "cost\n",
      "2.80484209408\n",
      "minibatch_index: 25\n",
      "cost\n",
      "2.6967335291\n",
      "minibatch_index: 26\n",
      "cost\n",
      "2.59995252161\n",
      "minibatch_index: 27\n",
      "cost\n",
      "2.52928238249\n",
      "minibatch_index: 28\n",
      "cost\n",
      "2.59878489989\n",
      "minibatch_index: 29\n",
      "cost\n",
      "2.75557227008\n",
      "minibatch_index: 30\n",
      "cost\n",
      "2.49882192961\n",
      "minibatch_index: 31\n",
      "cost\n",
      "2.58434166609\n",
      "minibatch_index: 32\n",
      "cost\n",
      "2.16192679234\n",
      "minibatch_index: 33\n",
      "cost\n",
      "2.46372970157\n",
      "minibatch_index: 34\n",
      "cost\n",
      "2.30682236527\n",
      "minibatch_index: 35\n",
      "cost\n",
      "2.42070464172\n",
      "minibatch_index: 36\n",
      "cost\n",
      "2.34862938837\n",
      "minibatch_index: 37\n",
      "cost\n",
      "2.35625076628\n",
      "minibatch_index: 38\n",
      "cost\n",
      "2.18991153808\n",
      "minibatch_index: 39\n",
      "cost\n",
      "2.42626889334\n",
      "minibatch_index: 40\n",
      "cost\n",
      "2.29812609564\n",
      "minibatch_index: 41\n",
      "cost\n",
      "2.33940653467\n",
      "minibatch_index: 42\n",
      "cost\n",
      "2.14037170377\n",
      "minibatch_index: 43\n",
      "cost\n",
      "2.3777242613\n",
      "minibatch_index: 44\n",
      "cost\n",
      "2.37278636576\n",
      "minibatch_index: 45\n",
      "cost\n",
      "2.30804061659\n",
      "minibatch_index: 46\n",
      "cost\n",
      "2.15491718137\n",
      "minibatch_index: 47\n",
      "cost\n",
      "2.08038897419\n",
      "minibatch_index: 48\n",
      "cost\n",
      "2.20366047184\n",
      "minibatch_index: 49\n",
      "cost\n",
      "2.35263884199\n",
      "minibatch_index: 50\n",
      "cost\n",
      "2.29980492441\n",
      "minibatch_index: 51\n",
      "cost\n",
      "2.37723801492\n",
      "minibatch_index: 52\n",
      "cost\n",
      "2.50796324984\n",
      "minibatch_index: 53\n",
      "cost\n",
      "2.17563697927\n",
      "minibatch_index: 54\n",
      "cost\n",
      "2.18023743609\n",
      "minibatch_index: 55\n",
      "cost\n",
      "2.19461598292\n",
      "minibatch_index: 56\n",
      "cost\n",
      "2.05204493517\n",
      "minibatch_index: 57\n",
      "cost\n",
      "2.04918548759\n",
      "minibatch_index: 58\n",
      "cost\n",
      "2.14085178471\n",
      "minibatch_index: 59\n",
      "cost\n",
      "2.15405907568\n",
      "minibatch_index: 60\n",
      "cost\n",
      "2.05493218131\n",
      "minibatch_index: 61\n",
      "cost\n",
      "1.83303287074\n",
      "minibatch_index: 62\n",
      "cost\n",
      "2.06444545603\n",
      "minibatch_index: 63\n",
      "cost\n",
      "2.0691170587\n",
      "minibatch_index: 64\n",
      "cost\n",
      "1.98887265418\n",
      "minibatch_index: 65\n",
      "cost\n",
      "1.8881805801\n",
      "minibatch_index: 66\n",
      "cost\n",
      "2.08333922039\n",
      "minibatch_index: 67\n",
      "cost\n",
      "1.99531016599\n",
      "minibatch_index: 68\n",
      "cost\n",
      "2.04607709567\n",
      "minibatch_index: 69\n",
      "cost\n",
      "2.21422080934\n",
      "minibatch_index: 70\n",
      "cost\n",
      "1.99001479192\n",
      "minibatch_index: 71\n",
      "cost\n",
      "1.93533399761\n",
      "minibatch_index: 72\n",
      "cost\n",
      "1.8205463084\n",
      "minibatch_index: 73\n",
      "cost\n",
      "2.08380019189\n",
      "minibatch_index: 74\n",
      "cost\n",
      "1.93499873121\n",
      "minibatch_index: 75\n",
      "cost\n",
      "1.95950633969\n",
      "minibatch_index: 76\n",
      "cost\n",
      "2.08760163286\n",
      "minibatch_index: 77\n",
      "cost\n",
      "1.8627825584\n",
      "minibatch_index: 78\n",
      "cost\n",
      "1.88433797433\n",
      "minibatch_index: 79\n",
      "cost\n",
      "1.77013532353\n",
      "minibatch_index: 80\n",
      "cost\n",
      "1.61233130461\n",
      "minibatch_index: 81\n",
      "cost\n",
      "1.88469516431\n",
      "minibatch_index: 82\n",
      "cost\n",
      "1.93176759198\n",
      "Optimization complete. Best validation score of 20.427083 % obtained at iteration 165, with test performance 11.968750 %\n"
     ]
    }
   ],
   "source": [
    "def train_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=2,\n",
    "         dataset='mnist.pkl.gz', batch_size=500, n_hidden=4):\n",
    "    #define hyper paramaters \n",
    "    print(\"start looping\")\n",
    "    ds = load_data('MNIST')\n",
    "    train_set_x , train_set_y, valid_set_x, valid_set_y, test_set_x, test_set_y, n_train_batches, n_valid_batches,n_test_batches = splitNum(ds , 600)\n",
    "    train_model , test_mdoel, valid_model = theanofunctions(600,train_set_x , train_set_y)\n",
    "    patience = 10  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = True\n",
    "    print(\"start looping\")\n",
    "    while (epoch < n_epochs) and ( done_looping):\n",
    "        epoch = epoch + 1\n",
    "        print(epoch)\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            print(\"minibatch_index: %d\" %( minibatch_index))\n",
    "            mb_avg_cost = train_model(minibatch_index)\n",
    "            print(\"cost\")\n",
    "            print(mb_avg_cost)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) %validation_frequency == 0: \n",
    "\n",
    "                validation_loss = [valid_model(i) for i in xrange(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_loss)\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    if this_validation_loss < best_validation_loss *improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter \n",
    "                    test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "                    test_score = numpy. mean(test_losses)\n",
    "            #if patience <= iter:\n",
    "                #done_looping = True\n",
    "                #break\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete. Best validation score of %f %% '\n",
    "    'obtained at iteration %i, with test performance %f %%') %\n",
    "    (best_validation_loss * 100., best_iter + 1, test_score * 100.))#this is measuring the losses, so cost goes fown losses go down\n",
    "train_mlp()\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start looping\n",
      "start looping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-6aa63e3d850e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_mlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-341c75d07a76>\u001b[0m in \u001b[0;36mtrain_mlp\u001b[1;34m(learning_rate, L1_reg, L2_reg, n_epochs, dataset, batch_size, n_hidden)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mepcoh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mmb_avg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0miter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mvalidation_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runcheck(minibatch_index):\n",
    "    mb_avg_cost = train_model(minibatch_index) #this function should have access to all of the theano functions\n",
    "    iter = (epoch - 1) * n_train_batches + minibatch_index#also n_train_batches,valid,test\n",
    "    if (iter + 1) %validation_frequency == 0: #also val freq\n",
    "\n",
    "        validation_loss = [validate_model(i) for i in xrange num_valid_batches] \n",
    "        this_val_loss = numpy.mean(validation_losses)\n",
    "        if this validation_loss < best_validation_loss: #also best_val_loss\n",
    "            if this_validation_loss < best_validation_loss *improvement_threshold):\n",
    "                patience = max(patience, iter * patience_increase)\n",
    "            best_validation_loss = this_validation_loss#\n",
    "            best_iter = iter \n",
    "            test_losses = [test_model(i) for i in xrange(num_test_batches)]\n",
    "            test_score = numpy. mean(test_losses)\n",
    "    if patience <= iter:\n",
    "        done_looping = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = load_data('MNIST')\n",
    "train_set_x , train_set_y, valid_set_x, valid_set_y, test_set_x, test_set_y, n_train_batches, n_valid_batches,n_test_batches = splitNum(ds , 600)\n",
    "train_model , test_mdoel, valid_model = theanofunctions(600,train_set_x , train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.matrix('x')#declare varaibles that will be stand in for data in theano functions\n",
    "y= T.ivector('y')\n",
    "index = T.lscalar()\n",
    "l1_reg=0.00\n",
    "l2_reg=0.0001\n",
    "rng = numpy.random.rand(1234) #3 seed the random state\n",
    "\n",
    "classifier = MLP(input = x, nin = 28*28, nhidden = 5, nout = 10, rng = rng) # use the MLP class\n",
    "cost = (classifier.neg_log_likelihood(y) + l1_reg * classifier.l1 + l2_reg*classifier.l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
