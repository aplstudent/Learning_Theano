-i wrote out the logistic regression class and tested it 
-i created simple variables and a simple function to test with
-i created a cost function and a gradient function
-I created a simple logistic regression class. The parts int's initiallized with weights and bias matrix, prediciton
-there is a cost function which in this cass is the negative log loss. 
-i created some fake datapoints and performed a linear regression on them. paramters, cost function, updates


-loaded the data from the pickled mnist file into a dataframe

February 5th
-i'm going to rearrange the multilayer perceptron so that it's in good object oriented programming. and i;ll rebuild it using the tutorial as a guide. 
#the ipython 3 kernel is dead and wont start 
-rewriting the code
-thte gradient is not computing it's asking for ndims to be 0 but classifier.l1 is 2 
-taking out the regularization. ill try to put it in later
-took out hidden layar params to see if it will work
-got  piece to work but now the next one isnt' 
February 6th
-i got it to work, but it's not looping through on every epoch 
-when optimization updates are param - lr*param the percentage correct went down, when it was positive it went up
-how do i vissualize what the outputs are. the cost i want to minimize so it should be minus the gradient can still be + or -
-i'm printing out cost, iteration number. I am running a test and taking these measurments on it. 
-i'm looking at the best_validation loss so i want it to below. 
-Have I hit the benchmark? what happens when i change hidden layers? how optimizable is this? features to add? features to take away
-I think a multilayer perceptron no bells or whistles is a good thing to write. cause i can strip down this one down
-multi-layer perceptron one node, one hidden layer 0-0-0, inputs, hidden , logistic
--regularization, l1 and l2
--drop out
--paramaterized hidden units
--paramaterized number of hidden layers
--multiple acitivation functions
--multiple regression classes
--AUC curve
--performance standards
--minibatch gradient descent
--number of epochs
--no validation checkor no test score check. 
--so this model will be drastically overfit 
--hard code everything all the paramaters, but identify them as hyper paramaters
--there are also regular paramaters 
--
but as a teaching mechanic I think that is fine
--i'm noting what i'm taking out
-- i striped down a lot of the functions , now i'm getting a type error for one of my y's. 
